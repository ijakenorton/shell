# A Survey on Data-driven Software Vulnerability Assessment and Prioritization

- Software vulnerabilities are increasing in complexity and scale
- Increase in machine learning for assessment and prioritization
- Also discusses current limitations

## Introduction

Can affect confidentiality, integrity and availability of software systems, such as Heartbleed
(example of attack), therefore important to remediate as promptly as possible

### Vulnerability Assessment

- More than 20000 SVs in NVD in 2021

Assessment entails:

- determining:
    - types
    - exploitability
    - impact
    - severity level
- These are needed to reason as to the level of prioritization needed for a given SV
    - Example given of a cross-site scripting attack needing to be resolved quickly---deemed as high
      threat due to only needing unauthorized access and so readily accessible---.
- Active research which is increasing. Generally making use of:
    - NLP
    - ML
    - DL
    to automate the prediction of CVSS metrics or public exploits.
- Models are able to do this by learning the patterns automatically from the vast SV data.---Unsure
  if I would call it vast from a ML standpoint, says impossible manually, but that is what is
currently done?---

### Key contributions:
- First to do in-depth review of studies that automate data-driven SV assessment and
prioritization tasks leveraging SV data and NLP/ML/DL techniques.
- Categorize the key tasks preformed in relevant primary studies
- Synthesize and discuss the pros and cons of data, features, models, evaluation methods, and
metrics commonly used in the reviewed studies
- Highlight the challenges with the current practices and propose potential solutions moving
forward.

Focus on the challenges, solutions and practices of automating various SV assessment and
prioritization tasks with data-driven techiniques

## Overview

Discovery phase:
    - locates hot spots that contain many highly critical/severe SVs and require higher attention
in a system
Assessment phase:
    - unveils the characteristics of the SVs
Prioritization phase:
    - practitioners use the assessment outputs to device and optimal remediation plan
Remediation phase:
    - Fix the things

--- In hindsight, this doesnt really feel like this was necessary. ---

Survey intends to focus on the SV data in the wild as opposed to the rule/experience based
assessment and prioritization. This paper only surveying papers which use data-driven methods/models, no manual analysis.

They don't directly compare the results as they do not necessarily match up.

## Methodology

Search research paper databases based of this big ol boolean string 
```
software AND vulner* AND (learn* OR data* OR predict*) AND (priority* OR assess* OR impact* OR exploit*
OR severity*) AND NOT (fuzz* OR dynamic* OR intrusion OR adversari* OR malware* OR vulnerability detection’ OR vulnerability discovery’ OR vulnerability identification’ OR vulnerability prediction’).
```
84 papers included

### Thematic analysis

Followed these steps to identify the taxonomy:
- Pilot study of 20 papers to identify the data to be extracted
- Generated initial codes??? 
- Merged interatively to create themes
- Two authors did the analysis independently and contrasted their findings

The five major themes are:

- Exploitation
    - Exploit likelihood
    - Exploit time
    - Exploit characteristics
- Impact
    - Confidentiality
    - Integrity
    - Availability
    - Scope
    - Custom Vulnerability Consequences
- Severity
    - Severe vs. Non-severe
    - Score
    - Levels
- Type
    - Common Weakness Enumeration (CWE)
- Miscellaneous tasks
    - Vulnerability Information Retrieval
    - Cross-source Vulnerability Patterns
    - Vulnerability Fixing Effort

## Exploitation Prediction

Focuses on automating the detection and understanding of both proof-of-concept and real-world
exploits, it outputs:

- Origin of SV
- How the SVs are taken advantage of by attackers
- Assisting security practitioners to react quicker to threats

### Exploit Likelihood

Predicts whether SVs would be exploited in the wild of PoC exploits would be released publicly

- Authors of various papers predicting PoC exploits urg explicitly considering real-world exploits,
not all PoC exploits would result in exploitation in practice
- Twitter data can aid in earlier detection than use of other sources, e.g. NVD

Use of many other security resources:

- Zero Dat Initialive,
- Metasploit,
- SecurityFocus,
- Recorded Future,
- Kenna Security,
- Avast,
- ESET,
- Tredn Micro,
- Malicious activities in hosts based on traffic of spam/malicious IP addresses ---Not sure what to make of
this---
- Darkweb sites/forums/markets

Many ensemble models used to make inferences on this data for prediction of real world exploits:

- Ramdon forest,
- eXtreme Gradient Boosting ( XGBoost ),
- Light Gradient Boosting Machine ( LGBM ),

These were shown to outperform single-model baselines.

Bullough et al. identified and addressed several issues with exploit prediction models:

- Time sensitivity of SV data,
- Already-exploited SVs before disclosure,
- Training data imbalance, 

***
Broadcom. [n. d.]. Symantec threat explorer. Retrieved from https://bit.ly/symantec_threats. [23] Benjamin L. Bullough, Anna K. Yanchenko, Christopher L. Smith, and Joseph R. Zipkin. 2017. Predicting exploitation of disclosed software vulnerabilities using open-source data. In Proceedings of the 3rd ACM on International Workshop on Security And Privacy Analytics. 45–53. 
***

Transfer learning is another possible solution improving performance of exploit prediction with
scarcely labeled exploits, what a surprise, a fine-tuned BERT model!

***
Adding additional context, i.e. the type of the exploit seemed to help model performance
[13] Navneet Bhatt, Adarsh Anand, and V. S. S. Yadavalli. 2021. Exploitability prediction of software vulnerabilities. Qual. Reliab. Eng. Int. 37, 2 (2021), 648–663. [14] Farzana Ahamed Bhuiyan, Md Bulbul Sharif, and Akond Rahman. 2021. Security bug report usage for software
***

--- This makes sense however I wonder how they attained this information ---

Various studies have predicted exploits on the code level. 
--- While this is interesting it isn't what I have looked into and probably waste of time looking into ---
--- On the other hand, at the point of when this paper was released, all of the papers are using
more basic stats techniques and so it could be possible that an updated look into this might be
interesting ---

### Exploit Time

This section focuses on trying to predict the time frames that the exploit will happen after the
disclosure --- Also not really relevant to me ---

### Exploit Characteristics

Reveals the requirements/means of exploits, related to the severity and score and often communicated
using the Common Vulnerability Scoring System (CVSS)

Many different models have been tried, these are collection of models for each metric:

- Latent Dirichlet Allocation topic model,
- Radial Basis Function-kernel SVM,

Le et at. showed that a lot of these models suffered from concept drift, descriptions of new SVs
may contain Out-of-Vocabulary terms for prediction models

- Sub-word features with traditional Bag-of-Word (BoW) features to infer the semantics of novel
terms/words from existing ones 
- Principal component analysis
- Random Forest
- Graph Convolutional Network using twitter data to be more timely
--- Unsure what model this was then applied to, probably not relevant ---

Some have also tried to do one model for all metrics for efficiency gain:

- Multiple CVSS metrics as a unique string,
- Multi-task learning paradigm to predict metrics simultaneously using Bi-LSTM with attention
    - Specific head/layer for each metric/task --- vague, says it outperformed, maybe it did at that
      point? --- 

Some authors persued alternatives to CVSS

These could include:
- SV locations, Local, LAN, Remote
- 8 Access/Input/Origin validation error, Atomicity/Configuration/Design/Environment/Serialization error, Boundary condition error, Failure on exceptions, Race conditions error.

Exploit type:
- LDA,
- Random Forest to classify if an exploit would affect a web application

Useful to find exploits in components/sub-systems of a large system

Privileges were also looked into and extended. Aksu et al. exteneded the privileges metric by
incorporating context (i.e. Operationg system or Application) to which privileges are applied? They
found MLP to be the best performing

Pushed further using CNN for feature extraction

Kanakogi for Doc2vec to be more effective than tf-idf to find the most relevant Common Attack
Pattern Enumeration and Classification (CAPEC) --- Haven't really looked into this one ---

### Theme 

Proof-of-Concept exploits are much more common than the real-world counter parts. As a result the
real-world predictions have usually performed worse --- less data --- 
The community is more interested in the real-world predictions.

Idea is to transfer what has been learned by the work on predicting proof of concept and move that
into the real-world

Some interest in adding additional information to the data, when and how the exploits happen etc...
Mentions VEST, --- not really interested in that ---

#### Description Related prediction

Surprised that people have been mainly using the descriptions from NVD etc, even though they do not
contain the root cause of the SV
--- Doesn't surprise me, a lot of the time the root cause in the source code is not known... Is that
true? ---

Little work on code-based exploit prediction, --- I thought that I had seen some of this but maybe it is
not very fleshed out? Or maybe it wasn't at this point. --- 
- Some work, however still requires some manual work of identification of dangerous function calls.
--- Does make me wonder if it would be possible to train a network on finding specific exploits.
Would require a dataset where known issues are found, buffer overflow etc ---

### Impact Predicition

Prediction of the negative effects that software vulnerabilities have on the systems that contain
them.

--- Unsure why this is a separate section really, it is just a specific part of what was already
discussed in the exploit section ---

Lack of comparison between ensemble models and DL ones perhaps could be interesting, but also, who
cares

#### Discussion

Keep mentioning that the models are having a harder time with CVSS 3 over 2. --- I guess maybe at this
point this was true? Though they are hard to compare really ---

### Severity Prediction

Function/combination of Exploitation and Impact

#### Severe vs Non-severe

Mostly who cares, but generally a bunch of standard stats/ML techniques 
- Bagging,
- Random forest,
- Naive Bayes,
- Support Vector Machine,

#### Severity Levels

Multi-class classification into three levels Low, Medium and High of CVSS or WIVSS

Other techniques include:
- XGBoost
- LGBM
- Recurrent Convolutional Neural Network (RCNN),
- Convolutional Neural Network,
- Long Short Term Memory

To everyones surprise, DL methods are generally an improvement over the previous ML methods

Nakagawa et al. enhanced the method by incorporating the character-level features into a CNN model

***
[215] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. arXiv preprint arXiv:1509.01626 (2015). [216] Yu Zhang, Peter Tiňo, Aleš Leonardis, and Ke Tang. 2020. A survey on neural network interpretability. arXiv preprint 
***


